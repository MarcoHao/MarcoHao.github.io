# 多视图 3D 模型生成工作流 Exploring Multi-view Consistency in AI-based 3D Generation

> Diffusion, AI, Hunyuan3d
> 
- 原版
    
    
    在ComfyUI上基于开源模型Hunyuan3d，实现从text2image到image2model的过程。通过IPAdapter、LoRA训练等方式，控制多视图生成中的特征连续，以ControlNet的串联来实现对模型的生成姿势的控制，最终通过基于DiT的多视角推理与纹理渲染之后，输出可用的3D模型，并通过3D打印方式测试了实际效果，打开了本地设计，生成，制造的大门。
    
    在视图控制上，通过对比单视图直接生成、多角度视图生成、一次性多角度视图生成三种方式，得出一次性生成多视图的结果控制最为合理，并且调参效率上更加占优的结论，并以此为基础开展后续工作。
    
    - 不同生成方式对结果的控制情况
        
        
        |  | 单视图推理 | 多视图分别推理 | 统一多视图推理 |
        | --- | --- | --- | --- |
        | 时长 |  |  |  |
        | 生成效果 |  |  |  |
    
    在不同生成方式上，直接从单一图片上生成多视角的人物模型相对来说更加简单和稳定，也更加容易保持多个视角的相近，并且比直接让Hunyuan3D进行单一图片单个视角的推理会获得更多的模型细节。分成多次单一视角生成的话，对于不做调整的模型来说略有困难，因此需要以主视角建立控制模型，使用IPAdapter来进行模型的调整，使得后续模型在一定程度上能反映主视角的特征，但是在连续性上仍然弱于单一图片内进行多视图推理，虽然在人物角度控制上更加容易。
    
    ControlNet的控制是生成多个视角的关键，通过调整OpenPose和DeepPose在生成过程的介入时域与权重，会直接影响多视图的生成结果。
    
    在3D模型生成上，以Hunyuan3D工作流为基础，通过微调参数取得更好的纹理生成效果，对比了不同的采样方法和参数设置带来的效果。
    
    - Delight模型面对不同的背景明度和不同的主题曝光程度的反映
        
        
        |  | 0.5 | 0.95 |
        | --- | --- | --- |
        | 主体过曝 |  |  |
        | 主体欠曝 |  |  |
    - 不同的采样方法对于Delight和Paint模型的影响
        
        
        |  | Delight | SampleMultiView |
        | --- | --- | --- |
        | Eula |  |  |
        | PDNM |  |  |
    
    最终在此基础上通过Hunyuan3DInpaint工作流来将生成的多视图转化为贴图赋予给生成的MESH，并输出为三维模型。
    

# **多视图 3D 模型生成工作流** *Exploring Multi-view Consistency in AI-based 3D Generation*

### ——从文本生成到三维重建的智能化探索

Diffusion, AI

**基于Hunyuan3D与ComfyUI框架，实现从 *text2image* 到 *image2model* 的全过程。项目聚焦于多视图一致性与可控性的核心问题，通过LoRA 微调与ControlNet 姿态控制，实现了稳定多视角 3D 生成流程，并以3D 打印验证完成从算法到制造的闭环。**

### **项目概述｜ Content**

本项目以 “多视图一致性” 为研究重点，旨在提升 AI 模型在三维生成中的结构稳定性与视觉连续性。在**ComfyUI**平台上构建以**Hunyuan3D**为核心的工作流，集成**IPAdapter、LoRA、ControlNet**

等模块，使系统能够在文本提示下自动生成多角度一致、姿态可控的图像序列，并进一步重建为三维模型。该工作流为后续的建模、纹理生成与制造验证提供了实验性基础，也验证了**设计推理 → 算法生成 → 实体输出**的完整路径。

### **工作流搭建与核心逻辑**

![workflow (3).png](workflow_(3).png)

📷 *ComfyUI 工作流示意*

在 ComfyUI 中构建完整节点网络，以 **Hunyuan3D** 作为生成核心，串联文本输入、IPAdapter 特征匹配、ControlNet 姿态控制、LoRA 权重微调等模块。

该工作流不仅支持多视图生成，也为模型控制提供了灵活接口，使得不同阶段可以针对性地调整控制强度与特征保持性。

### **多视图一致性实验**

📷 *视图控制对比实验*

为验证多视角生成效果，对比了三种生成方式：

1. 单视图独立推理
    
    ![单个视图生成效果3.png](%E5%8D%95%E4%B8%AA%E8%A7%86%E5%9B%BE%E7%94%9F%E6%88%90%E6%95%88%E6%9E%9C3.png)
    
2. 多视图分别推理
    
    ![多视图生成效果.png](%E5%A4%9A%E8%A7%86%E5%9B%BE%E7%94%9F%E6%88%90%E6%95%88%E6%9E%9C.png)
    
3. 一次性多视图推理
    
    ![单-多视图生成效果3.png](%E5%8D%95-%E5%A4%9A%E8%A7%86%E5%9B%BE%E7%94%9F%E6%88%90%E6%95%88%E6%9E%9C3.png)
    

实验结果表明：**一次性生成多视图** 的方式在控制一致性和参数效率上表现最优，能够更好地保持模型的整体结构和细节连续性。相较之下，多次独立推理虽能更灵活控制角度，但需要额外的特征校正，生成成本更高。

### **姿态控制与特征连续性**

📷 *ControlNet 控制效果对比*

![ComfyUI_00073_.png](ComfyUI_00073_.png)

![ComfyUI_00070_.png](ComfyUI_00070_.png)

多视图上，背部的生成是一大难点

在姿态控制方面，**ControlNet** 是多视图生成的关键环节。通过调整 **OpenPose** 与 **DeepPose** 的介入时域与权重，同时添加3D人物模型的LoRA，模型在姿态一致性与形体稳定性方面表现显著提升。同时，结合 **IPAdapter** 的主视角特征迁移，使不同角度的生成结果能够在视觉上保持统一风格。

---

### **纹理生成与采样优化**

📷 *纹理生成参数对比*

![参数对比_1.jpg](%E5%8F%82%E6%95%B0%E5%AF%B9%E6%AF%94_1.jpg)

![参数对比_2.jpg](%E5%8F%82%E6%95%B0%E5%AF%B9%E6%AF%94_2.jpg)

![参数对比_3.jpg](参数对比_3.png)

![参数对比_4.jpg](参数对比_4.png)

在生成阶段后期，通过调整采样方法与曝光控制，测试了 **Delight** 与 **SampleMultiView** 模型在不同明度、不同采样方法条件下的表现。对比发现，Delight 模型在中等曝光（约 0.95–0.98 区间）下能更稳定生成细腻的表面纹理，而不同的采样方法在平衡速度与质量方面存在差异，为后续渲染优化提供了参考。

### **结论**

该项目在多视图一致性、姿态可控性与纹理生成方面进行了系统性探索，构建了一个可扩展的 AI 生成工作流。研究提升了对 **Diffusion / DiT 模型原理** 的理解，展现了设计与算法的双向互动：以逻辑指导生成模型的构建，以算法思维拓展设计的表达边界。